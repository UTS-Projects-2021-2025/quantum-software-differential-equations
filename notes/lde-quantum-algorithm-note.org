#+title: LDE Quantum Algorithm Note

Let \(\F\) be a field (either \(\R\) or \(\C\)), and \(\F^{n}\) is a \(n\)-dimensional vector space over \(\F\).

* Summary and Key Ideas
  * Quantum computers can numerically solve LDE's in \(O(\log n)\) time where \(n\) is the dimension of a vector space, which takes \(O(n^{3})\) time classically.
  * This provides an exponential speed up which is due to quantum computers being able to efficiently solve systems of linear equations.
  * This speedup has some caveats, such as whether an exponential speedup for solving linear differential equations outweighs the costs of using quantum computers in the first place. Is it even that hard to solve LDE's classically or is the performance manageable (is waiting a couple hours for a computer to run ok).
  * How much time does it take to solve an LDE on a classical CAS like SageMath?
  * The cost of decomposing a non-unitary matrix as a product of unitaries. Computational cost of Gram-Schmidt?

* The Algorithm
Our goal is to solve the Harmonic oscilator equation using a quantum algorithm,

#+name: eqn:harmonic
\begin{equation}
f''(x) + \omega^{2}f(x) = 0
\end{equation}
with initial conditions \(f(x) = 1, f''(x) = 1\). This can be rewritten as follows,

Let \(\alpha = f(x)\) and \(\beta = f'(x)\). By rewriting Equation [[eqn:harmonic]] as \(f''(x) = \omega^{2}f(x)\) we obtain the following, \(\frac{d}{dx}\beta = f''(x) = -\omega f(x)\). For a vector \(v = \begin{bmatrix}\alpha \\ \beta \end{bmatrix}\), we obtain the following differential equation.

\begin{equation}
\frac{d}{dx} v = Av
\end{equation}

where

\begin{equation}
A = \begin{bmatrix}0 & 1\\ -\omega^{2} & 0\end{bmatrix}
\end{equation}

This equation can be solved symbolically to give us the following equation,

\begin{equation}
v(x) = e^{Ax}v(0)
\end{equation}
where \(v(0) = \begin{bmatrix}1 \\ 1\end{bmatrix}\) as per our initial equations.

Notice that when the Harmonic frequency \(\omega = 1\), our matrix \(A\) is unitary, in this case we denote the matrix \(A = U\).

We can numerically approximate \(v(x)\) to an approximation order \(k\) by truncating it's Taylor series as follows,

\begin{equation}
v(x) \approx \sum^{k}_{m=0} \frac{(Ut)^{m}}{m!}v(0)
\end{equation}

** The Quantum Circuit
Since our operator \(U\) is a unitary we don't need to worry about dealing with a non-unitary implementation of the quantum circuit making things significantly easier.


* Linear Differential Equations Background

Recall that a differential equation that is written in terms of it's derivative. For example Schrodinger's equation is a differential equation,

\begin{align}
i\hbar \frac{\partial}{\partial t} \ket{\Psi(t)} = \hat{H}\ket{\Psi(t)}
\end{align}

A linear differential equation (LDE) is an equation where each derivative

\begin{align}
c_{0}f(x) + c_{1}\frac{df(x)}{dx} + c_{2}\frac{df^{2}(x)}{dx^{2}}\cdots c_{k-1}\frac{df^{k-1}(x)}{dx^{k-1}} + c_{k} = 0
\end{align}

Solving LDE's is incredibly important in material science, physics and economics.

Since solving differential equations plays such an important role in many fields of mathematics, science and economics, there are several tools that are classically well suited to solving differential equations.

* Differential Equations and Computation

Differential equations form the backbone of many real world tasks in material science and quantitative finance. Therefore in computational algebra, there has been many different methods studied to efficiently solve differential equations. 

One of the first problems is understanding what it means to be /efficient/ in solving linear differential equations. There are many different approaches to solving LDE's where efficiency differs depending on the use case. They can be solved symbolically/algebraically or numerically for example.

There are several computer algebra systems that are quite good at solving differential equations, such as sagemaths, USyd Magma, Wolfram Mathematica and Matlab for example.

** Quantum Advantage


* Quantum Circuit for Solving first-order LDE's

Given a function \(f : \F \to \F^{n}\), a first order linear differential equation can be written as

\begin{align}
\frac{df(t)}{dt} = Af(t) + b
\end{align}
for some \(n\times n\) matrix \(A\) and a vector \(b\in \F^{n}\).
